{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Designing Data-Intensive Applications Alright, listen up bookworms! If you're part of the Recurse Center book club, I've got some juicy notes for you all about that DDIA book. You know, the one about data-intensive applications? Yeah, that's the one. So, get your reading glasses on and your thinking caps ready, because we're about to explore the wonderful world of data in a fun and informative way. Let's go! \"Designing Data-Intensive Applications\" by Martin Kleppmann is a comprehensive guide to the principles, challenges, and trade-offs involved in building data-intensive systems. The book is divided into three parts, covering the foundations of data systems, data storage and retrieval, and distributed systems. Here's a summary of each chapter: Part I: Foundations of Data Systems Chapter 1: Reliable, Scalable, and Maintainable Applications Introduces the three key attributes of good data systems: reliability, scalability, and maintainability Discusses the trade-offs and challenges involved in achieving these attributes Chapter 2: Data Models and Query Languages Discusses the different data models used in data systems, such as relational, document-oriented, and graph databases Introduces query languages used to retrieve data from these models Chapter 3: Storage and Retrieval Discusses different types of storage systems, such as file systems, relational databases, and NoSQL databases Introduces the concept of indexing and its importance in data retrieval Chapter 4: Encoding and Evolution Discusses the importance of data encoding and the challenges of evolving data formats over time Introduces different encoding formats, such as JSON, Protocol Buffers, and Avro Data Chapter 5: Replication Discusses the importance of replication for availability, fault tolerance, and scalability Introduces different replication strategies, such as single-leader and multi-leader replication Chapter 6: Partitioning Discusses the challenges of partitioning data in a distributed system Introduces different partitioning strategies, such as range partitioning and hash partitioning Chapter 7: Transactions Discusses the importance of transactions for data consistency in a distributed system Introduces different transaction models, such as two-phase commit and optimistic concurrency control Chapter 8: The Trouble with Distributed Systems Discusses the challenges and trade-offs involved in designing and operating distributed systems, such as network partitions and consistency trade-offs Part III: Derived Data Chapter 9: Batch Processing Introduces batch processing and the challenges of processing large amounts of data efficiently Introduces batch processing frameworks, such as Hadoop and Spark Chapter 10: Stream Processing Discusses the importance of stream processing for real-time data processing Introduces stream processing frameworks, such as Apache Kafka and Apache Flink Chapter 11: The Future of Data Systems Discusses the emerging trends in data systems, such as serverless computing and machine learning Discusses the challenges and opportunities presented by these trends Overall, \"Designing Data-Intensive Applications\" provides a comprehensive overview of the principles, challenges, and trade-offs involved in building data-intensive systems. The book is a must-read for anyone involved in designing, building, or operating data systems.","title":"Home"},{"location":"#designing-data-intensive-applications","text":"Alright, listen up bookworms! If you're part of the Recurse Center book club, I've got some juicy notes for you all about that DDIA book. You know, the one about data-intensive applications? Yeah, that's the one. So, get your reading glasses on and your thinking caps ready, because we're about to explore the wonderful world of data in a fun and informative way. Let's go! \"Designing Data-Intensive Applications\" by Martin Kleppmann is a comprehensive guide to the principles, challenges, and trade-offs involved in building data-intensive systems. The book is divided into three parts, covering the foundations of data systems, data storage and retrieval, and distributed systems. Here's a summary of each chapter: Part I: Foundations of Data Systems Chapter 1: Reliable, Scalable, and Maintainable Applications Introduces the three key attributes of good data systems: reliability, scalability, and maintainability Discusses the trade-offs and challenges involved in achieving these attributes Chapter 2: Data Models and Query Languages Discusses the different data models used in data systems, such as relational, document-oriented, and graph databases Introduces query languages used to retrieve data from these models Chapter 3: Storage and Retrieval Discusses different types of storage systems, such as file systems, relational databases, and NoSQL databases Introduces the concept of indexing and its importance in data retrieval Chapter 4: Encoding and Evolution Discusses the importance of data encoding and the challenges of evolving data formats over time Introduces different encoding formats, such as JSON, Protocol Buffers, and Avro Data Chapter 5: Replication Discusses the importance of replication for availability, fault tolerance, and scalability Introduces different replication strategies, such as single-leader and multi-leader replication Chapter 6: Partitioning Discusses the challenges of partitioning data in a distributed system Introduces different partitioning strategies, such as range partitioning and hash partitioning Chapter 7: Transactions Discusses the importance of transactions for data consistency in a distributed system Introduces different transaction models, such as two-phase commit and optimistic concurrency control Chapter 8: The Trouble with Distributed Systems Discusses the challenges and trade-offs involved in designing and operating distributed systems, such as network partitions and consistency trade-offs Part III: Derived Data Chapter 9: Batch Processing Introduces batch processing and the challenges of processing large amounts of data efficiently Introduces batch processing frameworks, such as Hadoop and Spark Chapter 10: Stream Processing Discusses the importance of stream processing for real-time data processing Introduces stream processing frameworks, such as Apache Kafka and Apache Flink Chapter 11: The Future of Data Systems Discusses the emerging trends in data systems, such as serverless computing and machine learning Discusses the challenges and opportunities presented by these trends Overall, \"Designing Data-Intensive Applications\" provides a comprehensive overview of the principles, challenges, and trade-offs involved in building data-intensive systems. The book is a must-read for anyone involved in designing, building, or operating data systems.","title":"Designing Data-Intensive Applications"},{"location":"chapter1/","text":"Reliable, Scalable, and Maintainable Applications NonFunctional Nonfunctional requirements include qualities such as performance, reliability, availability, scalability, maintainability, security, usability, and many others. These attributes are often critical for the success of a system and should be considered and prioritized alongside the system's functional requirements. Performance, for example, refers to the system's ability to respond to user requests within an acceptable time frame. Reliability refers to the system's ability to function correctly and consistently over time. Scalability refers to the system's ability to handle increasing amounts of work or traffic without a decrease in performance. Maintainability refers to the ease with which the system can be maintained, updated, or modified over time. Usability refers to the ease with which users can interact with the system and accomplish their tasks. Security refers to the system's ability to protect against unauthorized access or attacks. Nonfunctional requirements are often measured and evaluated using specific metrics or benchmarks, such as response time, availability percentage, or defect rate. These metrics can be used to track and monitor the system's performance over time and identify areas for improvement. Overall, nonfunctional requirements are an important aspect of system design and should be considered and prioritized alongside the system's functional requirements. Reliability In the context of building data-intensive applications, reliability refers to the ability of a system to operate correctly even in the presence of faults. Chapter 1 of \"Designing Data-Intensive Applications\" by Martin Kleppmann provides an overview of the techniques and technologies used to achieve reliability in data systems. The chapter discusses different types of faults that can occur in a system, such as hardware faults, software faults, and human errors. It introduces the concept of fault tolerance, which involves designing a system to continue functioning even when faults occur. One technique for achieving fault tolerance is replication, which involves creating multiple copies of data or processes and distributing them across multiple machines. The chapter discusses different replication strategies, such as single-leader replication and multi-leader replication. It also introduces the concept of quorums, which are used to ensure consistency between replicas. The chapter also discusses other techniques for achieving reliability, such as redundancy, isolation, and versioning. It introduces the CAP theorem, which states that it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance. The chapter explains the trade-offs involved in choosing between these attributes and discusses different strategies for balancing them. Overall, the Reliable section of \"Designing Data-Intensive Applications\" emphasizes the importance of reliability in data systems and introduces techniques for achieving fault tolerance and consistency. The section lays the foundation for the rest of the book, which goes into more detail on the different technologies and architectures used to build reliable, fault-tolerant data systems. scalability In the context of building data-intensive applications, scalability refers to the ability of a system to handle increasing amounts of work or traffic without a decrease in performance. Chapter 1 of \"Designing Data-Intensive Applications\" by Martin Kleppmann provides an overview of the techniques and technologies used to achieve scalability in data systems. The chapter discusses the challenges of achieving scalability, such as the need for parallelism, load balancing, and partitioning. It introduces the concept of vertical scaling, which involves increasing the resources of a single machine to handle more work, and horizontal scaling, which involves distributing work across multiple machines. The chapter discusses different types of partitioning strategies, such as range partitioning and hash partitioning. It explains how partitioning can help to achieve scalability by allowing data to be distributed across multiple machines and processed in parallel. The chapter also introduces the concept of load balancing, which involves distributing work evenly across multiple machines to avoid overloading any one machine. It discusses different load balancing strategies, such as round-robin and least connections, and explains how load balancing can help to achieve scalability and improve performance. Overall, the Scalability section of \"Designing Data-Intensive Applications\" emphasizes the importance of scalability in data systems and introduces techniques for achieving it. The section lays the foundation for the rest of the book, which goes into more detail on the different technologies and architectures used to build scalable, high-performance data systems. maintainability devops https://en.wikipedia.org/wiki/Spaghetti_code In the context of building data-intensive applications, maintainability refers to the ability of a system to be easily maintained and updated over time. Chapter 1 of \"Designing Data-Intensive Applications\" by Martin Kleppmann provides an overview of the techniques and technologies used to achieve maintainability in data systems. The chapter discusses the challenges of achieving maintainability, such as the need for modularity, abstraction, and encapsulation. It introduces the concept of software architecture, which involves breaking a system down into components that can be developed and maintained independently. It discusses different architectural styles, such as layered architecture and microservices architecture, and explains how they can help to achieve maintainability. The chapter also introduces the concept of version control, which is a technique for tracking changes to source code over time. It discusses different version control systems, such as Git and Subversion, and explains how they can help to achieve maintainability by making it easier to track and manage changes to a system's codebase. The chapter emphasizes the importance of testing and debugging in achieving maintainability. It introduces different types of testing, such as unit testing and integration testing, and explains how they can help to identify and prevent bugs in a system. It also discusses the importance of monitoring and logging, which can help to identify and diagnose problems in a running system. Overall, the Maintainability section of \"Designing Data-Intensive Applications\" emphasizes the importance of maintainability in data systems and introduces techniques for achieving it. The section lays the foundation for the rest of the book, which goes into more detail on the different technologies and architectures used to build maintainable, adaptable data systems.","title":"Chapter 1"},{"location":"chapter1/#nonfunctional","text":"Nonfunctional requirements include qualities such as performance, reliability, availability, scalability, maintainability, security, usability, and many others. These attributes are often critical for the success of a system and should be considered and prioritized alongside the system's functional requirements. Performance, for example, refers to the system's ability to respond to user requests within an acceptable time frame. Reliability refers to the system's ability to function correctly and consistently over time. Scalability refers to the system's ability to handle increasing amounts of work or traffic without a decrease in performance. Maintainability refers to the ease with which the system can be maintained, updated, or modified over time. Usability refers to the ease with which users can interact with the system and accomplish their tasks. Security refers to the system's ability to protect against unauthorized access or attacks. Nonfunctional requirements are often measured and evaluated using specific metrics or benchmarks, such as response time, availability percentage, or defect rate. These metrics can be used to track and monitor the system's performance over time and identify areas for improvement. Overall, nonfunctional requirements are an important aspect of system design and should be considered and prioritized alongside the system's functional requirements.","title":"NonFunctional"},{"location":"chapter1/#reliability","text":"In the context of building data-intensive applications, reliability refers to the ability of a system to operate correctly even in the presence of faults. Chapter 1 of \"Designing Data-Intensive Applications\" by Martin Kleppmann provides an overview of the techniques and technologies used to achieve reliability in data systems. The chapter discusses different types of faults that can occur in a system, such as hardware faults, software faults, and human errors. It introduces the concept of fault tolerance, which involves designing a system to continue functioning even when faults occur. One technique for achieving fault tolerance is replication, which involves creating multiple copies of data or processes and distributing them across multiple machines. The chapter discusses different replication strategies, such as single-leader replication and multi-leader replication. It also introduces the concept of quorums, which are used to ensure consistency between replicas. The chapter also discusses other techniques for achieving reliability, such as redundancy, isolation, and versioning. It introduces the CAP theorem, which states that it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance. The chapter explains the trade-offs involved in choosing between these attributes and discusses different strategies for balancing them. Overall, the Reliable section of \"Designing Data-Intensive Applications\" emphasizes the importance of reliability in data systems and introduces techniques for achieving fault tolerance and consistency. The section lays the foundation for the rest of the book, which goes into more detail on the different technologies and architectures used to build reliable, fault-tolerant data systems.","title":"Reliability"},{"location":"chapter1/#scalability","text":"In the context of building data-intensive applications, scalability refers to the ability of a system to handle increasing amounts of work or traffic without a decrease in performance. Chapter 1 of \"Designing Data-Intensive Applications\" by Martin Kleppmann provides an overview of the techniques and technologies used to achieve scalability in data systems. The chapter discusses the challenges of achieving scalability, such as the need for parallelism, load balancing, and partitioning. It introduces the concept of vertical scaling, which involves increasing the resources of a single machine to handle more work, and horizontal scaling, which involves distributing work across multiple machines. The chapter discusses different types of partitioning strategies, such as range partitioning and hash partitioning. It explains how partitioning can help to achieve scalability by allowing data to be distributed across multiple machines and processed in parallel. The chapter also introduces the concept of load balancing, which involves distributing work evenly across multiple machines to avoid overloading any one machine. It discusses different load balancing strategies, such as round-robin and least connections, and explains how load balancing can help to achieve scalability and improve performance. Overall, the Scalability section of \"Designing Data-Intensive Applications\" emphasizes the importance of scalability in data systems and introduces techniques for achieving it. The section lays the foundation for the rest of the book, which goes into more detail on the different technologies and architectures used to build scalable, high-performance data systems.","title":"scalability"},{"location":"chapter1/#maintainability","text":"devops https://en.wikipedia.org/wiki/Spaghetti_code In the context of building data-intensive applications, maintainability refers to the ability of a system to be easily maintained and updated over time. Chapter 1 of \"Designing Data-Intensive Applications\" by Martin Kleppmann provides an overview of the techniques and technologies used to achieve maintainability in data systems. The chapter discusses the challenges of achieving maintainability, such as the need for modularity, abstraction, and encapsulation. It introduces the concept of software architecture, which involves breaking a system down into components that can be developed and maintained independently. It discusses different architectural styles, such as layered architecture and microservices architecture, and explains how they can help to achieve maintainability. The chapter also introduces the concept of version control, which is a technique for tracking changes to source code over time. It discusses different version control systems, such as Git and Subversion, and explains how they can help to achieve maintainability by making it easier to track and manage changes to a system's codebase. The chapter emphasizes the importance of testing and debugging in achieving maintainability. It introduces different types of testing, such as unit testing and integration testing, and explains how they can help to identify and prevent bugs in a system. It also discusses the importance of monitoring and logging, which can help to identify and diagnose problems in a running system. Overall, the Maintainability section of \"Designing Data-Intensive Applications\" emphasizes the importance of maintainability in data systems and introduces techniques for achieving it. The section lays the foundation for the rest of the book, which goes into more detail on the different technologies and architectures used to build maintainable, adaptable data systems.","title":"maintainability"},{"location":"chapter2/","text":"Chapter 2 Data Models and Query Languages The limits of my language mean the limits of my world. \u2014Ludwig Wittgenstein, Tractatus Logico-Philosophicus (1922) Chapter 2 focuses on data models and query languages, which serve as the foundation for understanding how various databases and systems are designed to store, organize, and retrieve data. Data Models: Data models are abstract representations of how data is organized and structured. They are essential for ensuring that data is stored efficiently and can be easily accessed and manipulated. The chapter discusses three primary data models: a. Relational Model: Introduced by E.F. Codd in 1970, the relational model organizes data in tables (relations) consisting of rows (tuples) and columns (attributes). It is the basis for SQL databases, which are widely used in various applications. b. Document Model: This model stores data in semi-structured formats like JSON or XML. It is schema-less, allowing more flexibility in data representation and is often used in NoSQL databases like MongoDB and Couchbase. c. Graph Model: The graph model represents data as nodes (entities) and edges (relationships). It is particularly suitable for representing complex relationships, such as social networks or recommendation systems. Graph databases like Neo4j and ArangoDB utilize this model. Query Languages: Query languages are used to interact with databases and retrieve or manipulate data. They help bridge the gap between human-readable queries and low-level data storage. The chapter discusses several query languages for different data models: a. SQL (Structured Query Language): SQL is a declarative query language used primarily for relational databases. It allows users to specify what data they want, rather than how to retrieve it. b. MapReduce: A functional programming model used for processing large-scale data across distributed systems. It involves two primary functions - Map (transforming data) and Reduce (aggregating results). c. NoSQL Query Languages: Various NoSQL databases have their query languages, such as MongoDB's JSON-based query language or Cypher for Neo4j. d. GraphQL: A query language for APIs, allowing clients to request specific data from a server. It enables more efficient and flexible data retrieval. Relational Model Versus Document Model the section on the relational model versus the document model highlights the fundamental differences between these two data models and their respective use cases. Relational Model: The relational model organizes data in tables (relations) with rows (tuples) and columns (attributes). Each row represents an entity and columns represent its attributes. This model is based on a strict schema, ensuring consistency in data representation. Relational databases, such as PostgreSQL and MySQL, use SQL for querying data. Advantages: a. Schema enforcement, which ensures consistency. b. Support for complex joins, which enables querying related data across multiple tables. c. Wide adoption and mature ecosystem. Disadvantages: a. Limited flexibility in handling hierarchical and complex data structures. b. Potential performance issues with joins, especially for large-scale data. Document Model: The document model stores data in semi-structured formats, such as JSON or XML. It allows for schema-less or schema-flexible storage, providing more flexibility in data representation. Document databases, such as MongoDB and Couchbase, use specialized query languages tailored to their data structures. Advantages: a. Greater flexibility in handling diverse and evolving data structures. b. Better performance for read-heavy workloads and simpler data access patterns, as related data can be stored together in a single document. c. Easier to scale horizontally due to the denormalized nature of data storage. Disadvantages: a. Lack of schema enforcement can lead to data inconsistency. b. Limited support for complex joins, making it challenging to query related data across multiple documents. The choice between the relational and document model depends on the specific requirements of the application. The relational model is more suitable for applications that require strict consistency, complex querying, and transactional support. In contrast, the document model is more appropriate for applications that need flexibility in data representation, faster read operations, and simpler access patterns. Summary Data models are a huge subject, and in this chapter we have taken a quick look at a broad variety of different models. We didn\u2019t have space to go into all the details of each model, but hopefully the overview has been enough to whet your appetite to find out more about the model that best fits your application\u2019s requirements. Historically, data started out being represented as one big tree (the hierarchical model), but that wasn\u2019t good for representing many-to-many relationships, so the relational model was invented to solve that problem. More recently, developers found that some applications don\u2019t fit well in the relational model either. New nonrelational \u201cNoSQL\u201d datastores have diverged in two main directions: Document databases target use cases where data comes in self-contained docu\u2010 ments and relationships between one document and another are rare. Graph databases go in the opposite direction, targeting use cases where anything is potentially related to everything. Questions during review Declarative SQL (also known as the query language) is a way to describe what you want without specifying how to achieve it. In contrast, imperative SQL (also known as the data manipulation language) consists of statements that describe the actions you want to perform on the data. Here's an example of declarative SQL: -- Declarative SQL: Query to get the names of all employees who work in the 'Finance' department SELECT employees.name FROM employees JOIN departments ON employees.department_id = departments.id WHERE departments.name = 'Finance'; In this example, you describe what you want (names of employees in the Finance department) without specifying how to achieve it. The SQL engine is responsible for determining the best way to execute the query and retrieve the desired data. Here's an example of imperative SQL: -- Imperative SQL: Insert a new employee into the 'employees' table INSERT INTO employees (name, age, department_id) VALUES ('John Doe', 30, 1); -- Imperative SQL: Update an employee's name in the 'employees' table UPDATE employees SET name = 'Jane Doe' WHERE id = 1; -- Imperative SQL: Delete an employee from the 'employees' table DELETE FROM employees WHERE id = 1; In these imperative SQL examples, you are specifying the actions to perform on the data: insert a new employee, update an existing employee's name, and delete an employee. The SQL engine performs these actions directly on the data without having to determine how to achieve the desired outcome. Note that the distinction between declarative and imperative SQL is not strict, and the terms are not always used in the same way by everyone. The examples provided here are meant to illustrate the general idea of declarative SQL as focusing on the desired outcome without specifying how to achieve it, whereas imperative SQL focuses on the actions to be performed on the data. Further clarification Is it fair to say SQL is both imperative and declarative? Maybe not, maybe yes.. SQL is primarily a declarative language. However, when it comes to working with stored procedures or triggers, procedural SQL extensions like PL/SQL (in Oracle) or T-SQL (in SQL Server) are used, which have imperative programming constructs. These procedural extensions allow you to use control structures like loops, conditional statements, and error handling within the database environment, making it possible to create more complex, imperative-style logic. -- T-SQL: Create a stored procedure to update the salary of employees in a specific department CREATE PROCEDURE UpdateSalaryByDepartment @DepartmentName NVARCHAR(50), @SalaryIncrease DECIMAL(5, 2) AS BEGIN -- Declare a variable to hold the department ID DECLARE @DepartmentId INT; -- Get the department ID SELECT @DepartmentId = id FROM Departments WHERE name = @DepartmentName; -- Check if the department ID is not NULL IF @DepartmentId IS NOT NULL BEGIN -- Update the salary of employees in the specified department UPDATE Employees SET salary = salary * (1 + @SalaryIncrease) WHERE department_id = @DepartmentId; END ELSE BEGIN -- Raise an error if the department is not found RAISERROR('The specified department does not exist.', 16, 1); END END; In this T-SQL example, we create a stored procedure to update the salary of employees in a specific department. The procedure uses imperative constructs like variable declaration, conditional statements (IF-ELSE), and error handling (RAISERROR) to implement the logic. How is the data stored in nosql dB?","title":"Chapter 2"},{"location":"chapter2/#chapter-2","text":"Data Models and Query Languages The limits of my language mean the limits of my world. \u2014Ludwig Wittgenstein, Tractatus Logico-Philosophicus (1922) Chapter 2 focuses on data models and query languages, which serve as the foundation for understanding how various databases and systems are designed to store, organize, and retrieve data.","title":"Chapter 2"},{"location":"chapter2/#data-models","text":"Data models are abstract representations of how data is organized and structured. They are essential for ensuring that data is stored efficiently and can be easily accessed and manipulated. The chapter discusses three primary data models: a. Relational Model: Introduced by E.F. Codd in 1970, the relational model organizes data in tables (relations) consisting of rows (tuples) and columns (attributes). It is the basis for SQL databases, which are widely used in various applications. b. Document Model: This model stores data in semi-structured formats like JSON or XML. It is schema-less, allowing more flexibility in data representation and is often used in NoSQL databases like MongoDB and Couchbase. c. Graph Model: The graph model represents data as nodes (entities) and edges (relationships). It is particularly suitable for representing complex relationships, such as social networks or recommendation systems. Graph databases like Neo4j and ArangoDB utilize this model.","title":"Data Models:"},{"location":"chapter2/#query-languages","text":"Query languages are used to interact with databases and retrieve or manipulate data. They help bridge the gap between human-readable queries and low-level data storage. The chapter discusses several query languages for different data models: a. SQL (Structured Query Language): SQL is a declarative query language used primarily for relational databases. It allows users to specify what data they want, rather than how to retrieve it. b. MapReduce: A functional programming model used for processing large-scale data across distributed systems. It involves two primary functions - Map (transforming data) and Reduce (aggregating results). c. NoSQL Query Languages: Various NoSQL databases have their query languages, such as MongoDB's JSON-based query language or Cypher for Neo4j. d. GraphQL: A query language for APIs, allowing clients to request specific data from a server. It enables more efficient and flexible data retrieval.","title":"Query Languages:"},{"location":"chapter2/#relational-model-versus-document-model","text":"the section on the relational model versus the document model highlights the fundamental differences between these two data models and their respective use cases. Relational Model: The relational model organizes data in tables (relations) with rows (tuples) and columns (attributes). Each row represents an entity and columns represent its attributes. This model is based on a strict schema, ensuring consistency in data representation. Relational databases, such as PostgreSQL and MySQL, use SQL for querying data. Advantages: a. Schema enforcement, which ensures consistency. b. Support for complex joins, which enables querying related data across multiple tables. c. Wide adoption and mature ecosystem. Disadvantages: a. Limited flexibility in handling hierarchical and complex data structures. b. Potential performance issues with joins, especially for large-scale data. Document Model: The document model stores data in semi-structured formats, such as JSON or XML. It allows for schema-less or schema-flexible storage, providing more flexibility in data representation. Document databases, such as MongoDB and Couchbase, use specialized query languages tailored to their data structures.","title":"Relational Model Versus Document Model"},{"location":"chapter2/#advantages","text":"a. Greater flexibility in handling diverse and evolving data structures. b. Better performance for read-heavy workloads and simpler data access patterns, as related data can be stored together in a single document. c. Easier to scale horizontally due to the denormalized nature of data storage.","title":"Advantages:"},{"location":"chapter2/#disadvantages","text":"a. Lack of schema enforcement can lead to data inconsistency. b. Limited support for complex joins, making it challenging to query related data across multiple documents. The choice between the relational and document model depends on the specific requirements of the application. The relational model is more suitable for applications that require strict consistency, complex querying, and transactional support. In contrast, the document model is more appropriate for applications that need flexibility in data representation, faster read operations, and simpler access patterns.","title":"Disadvantages:"},{"location":"chapter2/#summary","text":"Data models are a huge subject, and in this chapter we have taken a quick look at a broad variety of different models. We didn\u2019t have space to go into all the details of each model, but hopefully the overview has been enough to whet your appetite to find out more about the model that best fits your application\u2019s requirements. Historically, data started out being represented as one big tree (the hierarchical model), but that wasn\u2019t good for representing many-to-many relationships, so the relational model was invented to solve that problem. More recently, developers found that some applications don\u2019t fit well in the relational model either. New nonrelational \u201cNoSQL\u201d datastores have diverged in two main directions: Document databases target use cases where data comes in self-contained docu\u2010 ments and relationships between one document and another are rare. Graph databases go in the opposite direction, targeting use cases where anything is potentially related to everything.","title":"Summary"},{"location":"chapter2/#questions-during-review","text":"Declarative SQL (also known as the query language) is a way to describe what you want without specifying how to achieve it. In contrast, imperative SQL (also known as the data manipulation language) consists of statements that describe the actions you want to perform on the data. Here's an example of declarative SQL: -- Declarative SQL: Query to get the names of all employees who work in the 'Finance' department SELECT employees.name FROM employees JOIN departments ON employees.department_id = departments.id WHERE departments.name = 'Finance'; In this example, you describe what you want (names of employees in the Finance department) without specifying how to achieve it. The SQL engine is responsible for determining the best way to execute the query and retrieve the desired data. Here's an example of imperative SQL: -- Imperative SQL: Insert a new employee into the 'employees' table INSERT INTO employees (name, age, department_id) VALUES ('John Doe', 30, 1); -- Imperative SQL: Update an employee's name in the 'employees' table UPDATE employees SET name = 'Jane Doe' WHERE id = 1; -- Imperative SQL: Delete an employee from the 'employees' table DELETE FROM employees WHERE id = 1; In these imperative SQL examples, you are specifying the actions to perform on the data: insert a new employee, update an existing employee's name, and delete an employee. The SQL engine performs these actions directly on the data without having to determine how to achieve the desired outcome. Note that the distinction between declarative and imperative SQL is not strict, and the terms are not always used in the same way by everyone. The examples provided here are meant to illustrate the general idea of declarative SQL as focusing on the desired outcome without specifying how to achieve it, whereas imperative SQL focuses on the actions to be performed on the data.","title":"Questions during review"},{"location":"chapter2/#further-clarification","text":"Is it fair to say SQL is both imperative and declarative? Maybe not, maybe yes.. SQL is primarily a declarative language. However, when it comes to working with stored procedures or triggers, procedural SQL extensions like PL/SQL (in Oracle) or T-SQL (in SQL Server) are used, which have imperative programming constructs. These procedural extensions allow you to use control structures like loops, conditional statements, and error handling within the database environment, making it possible to create more complex, imperative-style logic. -- T-SQL: Create a stored procedure to update the salary of employees in a specific department CREATE PROCEDURE UpdateSalaryByDepartment @DepartmentName NVARCHAR(50), @SalaryIncrease DECIMAL(5, 2) AS BEGIN -- Declare a variable to hold the department ID DECLARE @DepartmentId INT; -- Get the department ID SELECT @DepartmentId = id FROM Departments WHERE name = @DepartmentName; -- Check if the department ID is not NULL IF @DepartmentId IS NOT NULL BEGIN -- Update the salary of employees in the specified department UPDATE Employees SET salary = salary * (1 + @SalaryIncrease) WHERE department_id = @DepartmentId; END ELSE BEGIN -- Raise an error if the department is not found RAISERROR('The specified department does not exist.', 16, 1); END END; In this T-SQL example, we create a stored procedure to update the salary of employees in a specific department. The procedure uses imperative constructs like variable declaration, conditional statements (IF-ELSE), and error handling (RAISERROR) to implement the logic.","title":"Further clarification"},{"location":"chapter2/#how-is-the-data-stored-in-nosql-db","text":"","title":"How is the data stored in nosql dB?"},{"location":"chapter5/","text":"Part II: Distributed Data Big Picture https://github.com/donnemartin/system-design-primer#master-slave-replication Chapter 5: Replication 5.1 Leaders and Followers 5.2 Synchronous Versus Asynchronous Replication 5.3 Setting Up New Followers 5.4 Handling Node Outages 5.5 Implementation of Replication Logs 5.6 Problems with Replication Lag Reading Your Own Writes Monotonic Reads Consistent Prefix Reads 5.10 Solutions for Replication Lag Replication in Distributed Systems Replication : Keeping a copy of the same data on multiple machines connected via a network. Reasons for replication: a. Reduce latency by keeping data geographically close to users b. Increase availability by allowing the system to work despite part failures c. Increase read throughput by scaling out the number of machines serving read queries Assumptions: Each machine can hold a copy of the entire dataset. Chapter 6 discusses partitioning for larger datasets. Replication challenges arise when handling changes to replicated data. Three popular replication algorithms: a. Single-leader b. Multi-leader c. Leaderless replication Each has pros and cons, which are examined in detail. Replication trade-offs: a. Synchronous vs. asynchronous replication b. Handling failed replicas These are often database configuration options with similar principles across implementations. Replication principles haven't changed much since the 1970s due to fundamental network constraints. Developers often misunderstand issues such as eventual consistency . The chapter also discusses read-your-writes and monotonic reads guarantees . Single Leader Diagram +---------+ | | | User | [1] O < --------| +Picture| /|\\ | | | +---------+ | | | v | [2] +----------------------+ | | | Write/Read Request | | | +----------------------+ | v +-------+ | | | Change| +-------+ / \\ / \\ / \\ [3]+------------+ +---------------+ +---------------+ | Leader |->| Follower |->| Follower | | Replica | | Replica 1 | | Replica 2 | +------------+ +---------------+ +---------------+ \\ \\ v [4] O +-----------+ /|\\ | | | | End User | | | (Read | | | Replica) | +-----------+ User uploads a picture : The user adds a picture to their data. Write/Read Request : The user sends a write request to the Leader Replica to update the data with the new picture. Change propagation : The Leader Replica processes the change and propagates it to the Follower Replicas (Follower Replica 1 and Follower Replica 2). End User reads data : The end user reads the updated data, including the newly added picture, from the selected Follower Replica (in this case, Follower Replica 1). 5.1 Leaders and Followers: The chapter begins by introducing the concept of leader-based replication, where one node is responsible for accepting writes and replicating changes to the followers to ensure data consistency. 5.2 Synchronous Versus Asynchronous Replication: Synchronous replication : Strong consistency across replicas. Higher latency for write operations. Lower fault tolerance. Asynchronous replication : Lower latency for write operations. Weaker consistency across replicas. Higher fault tolerance. The choice between synchronous and asynchronous replication depends on the specific requirements of the system and the desired balance between consistency, performance, and fault tolerance. 5.3 Setting Up New Followers: Snapshot : Take a consistent snapshot of the leader's database without locking the entire database. Copy : Transfer the snapshot to the new follower node. Connect and Request : The follower connects to the leader and requests data changes since the snapshot, using a specific position in the leader's replication log. Catch Up : The follower processes the backlog of data changes and continues to process new changes from the leader as they occur. The practical steps for setting up a follower vary by database. Some systems automate the process, while others require manual execution by an administrator. 5.4 Handling Node Outages: Follower failure: Catch-up recovery : Followers can recover from a crash or temporary network interruption using their local log of data changes. They reconnect to the leader and request all data changes that occurred during their disconnection. Leader failure: Failover : Promoting a follower to be the new leader, reconfiguring clients to send writes to the new leader, and ensuring other followers consume data changes from the new leader. Failover can be manual or automatic, and involves: Determining leader failure: Typically using a timeout. Choosing a new leader: Through an election process or appointing a new leader. Reconfiguring the system: Clients send write requests to the new leader, and the old leader must become a follower recognizing the new leader. Failover challenges : Asynchronous replication can result in unreplicated writes or data loss; discarding writes can lead to inconsistency; split-brain scenarios can cause data loss or corruption; and choosing the right timeout for leader failure detection can impact recovery time and risk unnecessary failovers. Some operations teams prefer manual failovers due to these challenges. These issues are fundamental problems in distributed systems and are discussed in greater depth in Chapters 8 and 9. 5.5 Implementation of Replication Logs: Replication logs serve as the foundation for leader-based replication in distributed systems, ensuring data consistency across nodes. This summary is based on the book \"Designing Data-Intensive Applications\" and is tailored for software engineers. +-----------+ +-----------+ +-----------+ | Leader | ----> | Follower | ----> | Follower | | | | | | | +----+------+ +-----+-----+ +-----+-----+ | | | v v v +------------+ +------------+ +------------+ | Replication | | Replication | | Replication | | Log | | Log | | Log | +------------+ +------------+ +------------+ Replication logs are sequential records of updates in a distributed system. The book covers four types of replication logs: Statement-based replication: Log Index | SQL Statement ------------------------- 1 | INSERT INTO users (id, name) VALUES (1, 'Alice') 2 | UPDATE users SET name = 'Alicia' WHERE id = 1 3 | DELETE FROM users WHERE id = 2 Records SQL statements executed on the primary node and replicates them to secondary nodes. Suitable for simple applications with deterministic SQL statements and minimal data transfer requirements. Write-ahead log (WAL) shipping: Log Index | Operation | Block | Offset | Data --------------------------------------------- 1 | INSERT | 5 | 1024 | {record_data} 2 | UPDATE | 5 | 1024 | {new_record_data} 3 | DELETE | 5 | 1024 | Sends the primary node's WAL to secondary nodes, which replay the log to apply changes. Ensures strong consistency and data durability , ideal for PostgreSQL-based systems and those prioritizing minimal impact on primary node performance. Logical (row-based) log replication: Log Index | Operation | Table | Row Data ----------------------------------------- 1 | INSERT | users | {id: 1, name: 'Alice'} 2 | UPDATE | users | {id: 1, name: 'Alicia'} 3 | DELETE | users | {id: 2} Captures changes made to individual rows in the primary node's data. Suitable for applications with non-deterministic operations , prioritizing performance , and requiring cross-platform compatibility . Trigger-based replication: Log Index | Trigger Event | Table | Old Row Data | New Row Data --------------------------------------------------------------- 1 | INSERT | users | | {id: 1, name: 'Alice'} 2 | UPDATE | users | {id: 1, name: 'Alice'} | {id: 1, name: 'Alicia'} 3 | DELETE | users | {id: 2} | Uses custom triggers to capture and propagate changes. Ideal for custom replication solutions , real-time data processing or transformation , and systems with heterogeneous databases. The choice of replication method depends on system requirements and constraints, such as consistency, performance, compatibility, and complexity . 5.6 Problems with Replication Lag: Replication is used for high availability , scalability , and reduced latency Leader-based replication requires writes to go through a single node, but read-only queries can go to any replica For workloads that consist mostly of reads and few writes, adding followers can increase the capacity for serving read-only requests This approach only works with asynchronous replication because synchronous replication would be unreliable Asynchronous replication can lead to eventual consistency , where followers may fall behind and return outdated information Replication lag can cause inconsistencies between the leader and followers Replication lag can increase with high system load or network problems, causing real problems for applications Reading Your Own Writes: Asynchronous replication can cause new data to not yet have reached the replica when the user views it shortly after making a write Read-after-write consistency guarantees that the user will always see any updates they submitted themselves Implementing read-after-write consistency in a system with leader-based replication requires various techniques, such as reading from the leader for user-modifiable data or tracking the time of the last update Replicas distributed across multiple datacenters or accessed from multiple devices add complexity and require centralized metadata or routing requests to the datacenter containing the leader Another complication arises when the same user is accessing your service from multiple devices, for example a desktop web browser and a mobile app. In this case you may want to provide cross-device read-after-write consistency Links to relevant Wikipedia articles: - Asynchronous replication - Consistency model - Data center Monotonic Reads: Reading from different asynchronous replicas can lead to a user seeing things moving backward in time Monotonic reads guarantee that time does not go backward when making multiple reads in sequence Monotonic reads are a lesser guarantee than strong consistency but stronger than eventual consistency One way to achieve monotonic reads is to make sure each user always makes their reads from the same replica, chosen based on a hash of the user ID Follower with little lag Follower with greater lag +-------------------------+ +-------------------------+ | | | | | User 2345 reads comment | | | | added by User 1234 | | | | Result A | | | | | | | +-------------------------+ | | | | | | | User 2345 reads comment | | added by User 1234 | | No result returned | | | +-------------------------+ This diagram represents the scenario where a user (User 2345) makes two queries to two different asynchronous replicas with different replication lags. The first query (Result A) is to a follower with little lag, and returns a comment added by another user (User 1234). The second query is to a follower with greater lag, and returns no results. This leads to the impression that time has gone backward for the user. To prevent this anomaly, we need monotonic reads. One way of achieving monotonic reads is to make sure that each user always makes their reads from the same replica (different users can read from different replicas). For example, the replica can be chosen based on a hash of the user ID, rather than randomly. However, if that replica fails, the user\u2019s queries will need to be rerouted to another replica. Monotonic reads can be achieved by ensuring that each user always reads from the same replica. This can be implemented through: Hash-based partitioning: Data is partitioned across replicas based on a hash of the user ID, ensuring that each user always reads from the same replica. Sticky sessions: A load balancer can use sticky sessions to route requests from a user to the same replica based on their initial request. Client-side routing: The client maintains state about the replicas and chooses the replica with the lowest replication lag for the user's reads. Metadata service: A metadata service maintains a mapping of user IDs to replica IDs and returns the ID of the replica that the user should read from. The mapping of users to replicas must be consistent and not change frequently to avoid inconsistent results. Consistent Prefix Reads: Inconsistent replication lag can lead to violations of causality, where reads may appear to happen before writes. This can be prevented by ensuring consistent prefix reads, which guarantees that if a sequence of writes occurs in a certain order, anyone reading those writes will see them appear in the same order. In partitioned databases, different partitions may operate independently, making it challenging to ensure consistent prefix reads. One solution is to write causally related writes to the same partition, but this may not always be efficient. Algorithms that track causal dependencies can also help prevent causality violations. 5.10 Solutions for Replication Lag: eventually consistent systems The chapter presents various techniques for addressing replication lag and improving data consistency, such as read-after-write consistency, quorum reads and writes, and version vectors. Read-after-write consistency: This approach ensures that when a client writes data to the system, it is only acknowledged once the write is propagated to a certain number of replicas. This guarantees that subsequent reads by the same client will return the latest written data. However, it may introduce latency during write operations, as the system needs to wait for the data to propagate. Quorum reads and writes: Quorum-based systems rely on a consensus algorithm to determine the number of replicas that must acknowledge a read or write operation before it is considered successful. A common approach is to use a majority quorum, where an operation must be acknowledged by more than half of the replicas to succeed. This method helps maintain consistency, as it is less likely that an outdated replica will be used for read operations. However, it may introduce latency, as the system needs to wait for multiple acknowledgments. Version vectors: Version vectors are data structures that keep track of the version history of each replica. They allow the system to detect and resolve conflicts arising from concurrent updates to the same data. By maintaining version information for each replica, the system can determine which replica has the most up-to-date data and use that replica for read operations. This approach can help address replication lag by ensuring that clients always read the latest data, even in the presence of network delays or other issues. Each of these techniques comes with its own trade-offs in terms of performance, latency, and complexity. The choice of which method to use depends on the specific requirements and characteristics of the distributed system being designed. Further Ideas https://github.com/donnemartin/system-design-primer#replication Resources Design Consitent Hashing - https://systemdesign.one/consistent-hashing-explained/","title":"Chapter 3"},{"location":"chapter5/#part-ii-distributed-data","text":"","title":"Part II: Distributed Data"},{"location":"chapter5/#big-picture","text":"https://github.com/donnemartin/system-design-primer#master-slave-replication","title":"Big Picture"},{"location":"chapter5/#chapter-5-replication","text":"5.1 Leaders and Followers 5.2 Synchronous Versus Asynchronous Replication 5.3 Setting Up New Followers 5.4 Handling Node Outages 5.5 Implementation of Replication Logs 5.6 Problems with Replication Lag Reading Your Own Writes Monotonic Reads Consistent Prefix Reads 5.10 Solutions for Replication Lag","title":"Chapter 5: Replication"},{"location":"chapter5/#replication-in-distributed-systems","text":"Replication : Keeping a copy of the same data on multiple machines connected via a network. Reasons for replication: a. Reduce latency by keeping data geographically close to users b. Increase availability by allowing the system to work despite part failures c. Increase read throughput by scaling out the number of machines serving read queries Assumptions: Each machine can hold a copy of the entire dataset. Chapter 6 discusses partitioning for larger datasets. Replication challenges arise when handling changes to replicated data. Three popular replication algorithms: a. Single-leader b. Multi-leader c. Leaderless replication Each has pros and cons, which are examined in detail. Replication trade-offs: a. Synchronous vs. asynchronous replication b. Handling failed replicas These are often database configuration options with similar principles across implementations. Replication principles haven't changed much since the 1970s due to fundamental network constraints. Developers often misunderstand issues such as eventual consistency . The chapter also discusses read-your-writes and monotonic reads guarantees .","title":"Replication in Distributed Systems"},{"location":"chapter5/#single-leader-diagram","text":"+---------+ | | | User | [1] O < --------| +Picture| /|\\ | | | +---------+ | | | v | [2] +----------------------+ | | | Write/Read Request | | | +----------------------+ | v +-------+ | | | Change| +-------+ / \\ / \\ / \\ [3]+------------+ +---------------+ +---------------+ | Leader |->| Follower |->| Follower | | Replica | | Replica 1 | | Replica 2 | +------------+ +---------------+ +---------------+ \\ \\ v [4] O +-----------+ /|\\ | | | | End User | | | (Read | | | Replica) | +-----------+ User uploads a picture : The user adds a picture to their data. Write/Read Request : The user sends a write request to the Leader Replica to update the data with the new picture. Change propagation : The Leader Replica processes the change and propagates it to the Follower Replicas (Follower Replica 1 and Follower Replica 2). End User reads data : The end user reads the updated data, including the newly added picture, from the selected Follower Replica (in this case, Follower Replica 1).","title":"Single Leader Diagram"},{"location":"chapter5/#51-leaders-and-followers","text":"The chapter begins by introducing the concept of leader-based replication, where one node is responsible for accepting writes and replicating changes to the followers to ensure data consistency.","title":"5.1 Leaders and Followers:"},{"location":"chapter5/#52-synchronous-versus-asynchronous-replication","text":"Synchronous replication : Strong consistency across replicas. Higher latency for write operations. Lower fault tolerance. Asynchronous replication : Lower latency for write operations. Weaker consistency across replicas. Higher fault tolerance. The choice between synchronous and asynchronous replication depends on the specific requirements of the system and the desired balance between consistency, performance, and fault tolerance.","title":"5.2 Synchronous Versus Asynchronous Replication:"},{"location":"chapter5/#53-setting-up-new-followers","text":"Snapshot : Take a consistent snapshot of the leader's database without locking the entire database. Copy : Transfer the snapshot to the new follower node. Connect and Request : The follower connects to the leader and requests data changes since the snapshot, using a specific position in the leader's replication log. Catch Up : The follower processes the backlog of data changes and continues to process new changes from the leader as they occur. The practical steps for setting up a follower vary by database. Some systems automate the process, while others require manual execution by an administrator.","title":"5.3 Setting Up New Followers:"},{"location":"chapter5/#54-handling-node-outages","text":"Follower failure: Catch-up recovery : Followers can recover from a crash or temporary network interruption using their local log of data changes. They reconnect to the leader and request all data changes that occurred during their disconnection. Leader failure: Failover : Promoting a follower to be the new leader, reconfiguring clients to send writes to the new leader, and ensuring other followers consume data changes from the new leader. Failover can be manual or automatic, and involves: Determining leader failure: Typically using a timeout. Choosing a new leader: Through an election process or appointing a new leader. Reconfiguring the system: Clients send write requests to the new leader, and the old leader must become a follower recognizing the new leader. Failover challenges : Asynchronous replication can result in unreplicated writes or data loss; discarding writes can lead to inconsistency; split-brain scenarios can cause data loss or corruption; and choosing the right timeout for leader failure detection can impact recovery time and risk unnecessary failovers. Some operations teams prefer manual failovers due to these challenges. These issues are fundamental problems in distributed systems and are discussed in greater depth in Chapters 8 and 9.","title":"5.4 Handling Node Outages:"},{"location":"chapter5/#55-implementation-of-replication-logs","text":"Replication logs serve as the foundation for leader-based replication in distributed systems, ensuring data consistency across nodes. This summary is based on the book \"Designing Data-Intensive Applications\" and is tailored for software engineers. +-----------+ +-----------+ +-----------+ | Leader | ----> | Follower | ----> | Follower | | | | | | | +----+------+ +-----+-----+ +-----+-----+ | | | v v v +------------+ +------------+ +------------+ | Replication | | Replication | | Replication | | Log | | Log | | Log | +------------+ +------------+ +------------+ Replication logs are sequential records of updates in a distributed system. The book covers four types of replication logs: Statement-based replication: Log Index | SQL Statement ------------------------- 1 | INSERT INTO users (id, name) VALUES (1, 'Alice') 2 | UPDATE users SET name = 'Alicia' WHERE id = 1 3 | DELETE FROM users WHERE id = 2 Records SQL statements executed on the primary node and replicates them to secondary nodes. Suitable for simple applications with deterministic SQL statements and minimal data transfer requirements. Write-ahead log (WAL) shipping: Log Index | Operation | Block | Offset | Data --------------------------------------------- 1 | INSERT | 5 | 1024 | {record_data} 2 | UPDATE | 5 | 1024 | {new_record_data} 3 | DELETE | 5 | 1024 | Sends the primary node's WAL to secondary nodes, which replay the log to apply changes. Ensures strong consistency and data durability , ideal for PostgreSQL-based systems and those prioritizing minimal impact on primary node performance. Logical (row-based) log replication: Log Index | Operation | Table | Row Data ----------------------------------------- 1 | INSERT | users | {id: 1, name: 'Alice'} 2 | UPDATE | users | {id: 1, name: 'Alicia'} 3 | DELETE | users | {id: 2} Captures changes made to individual rows in the primary node's data. Suitable for applications with non-deterministic operations , prioritizing performance , and requiring cross-platform compatibility . Trigger-based replication: Log Index | Trigger Event | Table | Old Row Data | New Row Data --------------------------------------------------------------- 1 | INSERT | users | | {id: 1, name: 'Alice'} 2 | UPDATE | users | {id: 1, name: 'Alice'} | {id: 1, name: 'Alicia'} 3 | DELETE | users | {id: 2} | Uses custom triggers to capture and propagate changes. Ideal for custom replication solutions , real-time data processing or transformation , and systems with heterogeneous databases. The choice of replication method depends on system requirements and constraints, such as consistency, performance, compatibility, and complexity .","title":"5.5 Implementation of Replication Logs:"},{"location":"chapter5/#56-problems-with-replication-lag","text":"Replication is used for high availability , scalability , and reduced latency Leader-based replication requires writes to go through a single node, but read-only queries can go to any replica For workloads that consist mostly of reads and few writes, adding followers can increase the capacity for serving read-only requests This approach only works with asynchronous replication because synchronous replication would be unreliable Asynchronous replication can lead to eventual consistency , where followers may fall behind and return outdated information Replication lag can cause inconsistencies between the leader and followers Replication lag can increase with high system load or network problems, causing real problems for applications","title":"5.6 Problems with Replication Lag:"},{"location":"chapter5/#reading-your-own-writes","text":"Asynchronous replication can cause new data to not yet have reached the replica when the user views it shortly after making a write Read-after-write consistency guarantees that the user will always see any updates they submitted themselves Implementing read-after-write consistency in a system with leader-based replication requires various techniques, such as reading from the leader for user-modifiable data or tracking the time of the last update Replicas distributed across multiple datacenters or accessed from multiple devices add complexity and require centralized metadata or routing requests to the datacenter containing the leader Another complication arises when the same user is accessing your service from multiple devices, for example a desktop web browser and a mobile app. In this case you may want to provide cross-device read-after-write consistency Links to relevant Wikipedia articles: - Asynchronous replication - Consistency model - Data center","title":"Reading Your Own Writes:"},{"location":"chapter5/#monotonic-reads","text":"Reading from different asynchronous replicas can lead to a user seeing things moving backward in time Monotonic reads guarantee that time does not go backward when making multiple reads in sequence Monotonic reads are a lesser guarantee than strong consistency but stronger than eventual consistency One way to achieve monotonic reads is to make sure each user always makes their reads from the same replica, chosen based on a hash of the user ID Follower with little lag Follower with greater lag +-------------------------+ +-------------------------+ | | | | | User 2345 reads comment | | | | added by User 1234 | | | | Result A | | | | | | | +-------------------------+ | | | | | | | User 2345 reads comment | | added by User 1234 | | No result returned | | | +-------------------------+ This diagram represents the scenario where a user (User 2345) makes two queries to two different asynchronous replicas with different replication lags. The first query (Result A) is to a follower with little lag, and returns a comment added by another user (User 1234). The second query is to a follower with greater lag, and returns no results. This leads to the impression that time has gone backward for the user. To prevent this anomaly, we need monotonic reads. One way of achieving monotonic reads is to make sure that each user always makes their reads from the same replica (different users can read from different replicas). For example, the replica can be chosen based on a hash of the user ID, rather than randomly. However, if that replica fails, the user\u2019s queries will need to be rerouted to another replica. Monotonic reads can be achieved by ensuring that each user always reads from the same replica. This can be implemented through: Hash-based partitioning: Data is partitioned across replicas based on a hash of the user ID, ensuring that each user always reads from the same replica. Sticky sessions: A load balancer can use sticky sessions to route requests from a user to the same replica based on their initial request. Client-side routing: The client maintains state about the replicas and chooses the replica with the lowest replication lag for the user's reads. Metadata service: A metadata service maintains a mapping of user IDs to replica IDs and returns the ID of the replica that the user should read from. The mapping of users to replicas must be consistent and not change frequently to avoid inconsistent results.","title":"Monotonic Reads:"},{"location":"chapter5/#consistent-prefix-reads","text":"Inconsistent replication lag can lead to violations of causality, where reads may appear to happen before writes. This can be prevented by ensuring consistent prefix reads, which guarantees that if a sequence of writes occurs in a certain order, anyone reading those writes will see them appear in the same order. In partitioned databases, different partitions may operate independently, making it challenging to ensure consistent prefix reads. One solution is to write causally related writes to the same partition, but this may not always be efficient. Algorithms that track causal dependencies can also help prevent causality violations.","title":"Consistent Prefix Reads:"},{"location":"chapter5/#510-solutions-for-replication-lag","text":"eventually consistent systems The chapter presents various techniques for addressing replication lag and improving data consistency, such as read-after-write consistency, quorum reads and writes, and version vectors. Read-after-write consistency: This approach ensures that when a client writes data to the system, it is only acknowledged once the write is propagated to a certain number of replicas. This guarantees that subsequent reads by the same client will return the latest written data. However, it may introduce latency during write operations, as the system needs to wait for the data to propagate. Quorum reads and writes: Quorum-based systems rely on a consensus algorithm to determine the number of replicas that must acknowledge a read or write operation before it is considered successful. A common approach is to use a majority quorum, where an operation must be acknowledged by more than half of the replicas to succeed. This method helps maintain consistency, as it is less likely that an outdated replica will be used for read operations. However, it may introduce latency, as the system needs to wait for multiple acknowledgments. Version vectors: Version vectors are data structures that keep track of the version history of each replica. They allow the system to detect and resolve conflicts arising from concurrent updates to the same data. By maintaining version information for each replica, the system can determine which replica has the most up-to-date data and use that replica for read operations. This approach can help address replication lag by ensuring that clients always read the latest data, even in the presence of network delays or other issues. Each of these techniques comes with its own trade-offs in terms of performance, latency, and complexity. The choice of which method to use depends on the specific requirements and characteristics of the distributed system being designed.","title":"5.10 Solutions for Replication Lag:"},{"location":"chapter5/#further-ideas","text":"https://github.com/donnemartin/system-design-primer#replication","title":"Further Ideas"},{"location":"chapter5/#resources","text":"Design Consitent Hashing - https://systemdesign.one/consistent-hashing-explained/","title":"Resources"}]}